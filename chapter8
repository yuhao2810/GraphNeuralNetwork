import os
import urllib
import torch
import torch.nn as nn

import torch.nn.functional as F
import torch.nn.init as init
import torch.utils.data as data

import numpy as np

import scipy.sparse as sp
from zipfile  import  ZipFile
from sklearn.model_selection import train_test_split
import pickle

'81'
def top_rank(att_score, graph_idicator ,keep_ratio):
    graph_id_list=list(set(graph_idicator.cpu().numpy()))
    mask=att_score.new_empty((0,),dtype=torch.bool)
    for graph_id in graph_id_list:
        graph_att_score=att_score[graph_idicator==graph_id]
        graph_node_num=len(graph_att_score)


        graph_mask=att_score.new_zeros((graph_node_num),dtype=torch.bool)
        keep_graph_node_num=int(keep_ratio*graph_node_num)
        _,sorted_index=graph_att_score.sort(descending=True)
        graph_mask[sorted_index[:keep_graph_node_num]]=True
        mask=torch.cat((mask,graph_mask))

    return mask

'82'
def normalization(adj):
    adj+=sp.eye(adj.shape[0])

    degree=np.array(adj.sum(1))
    d_hat=sp.diags(np.power(degree,-0.5).flatten())
    L=d_hat.dot(adj).dot(d_hat).tocoo()

    indices=torch.from_numpy(np.asarray([L.row,L.col])).long()
    values=torch.from_numpy(L.data.astype(np.float32))
    tensor_adj=torch.sparse.FloatTensor(indices,values,L.shape)

    return tensor_adj

def filter_adj(adj,mask):
    device=adj.device
    mask=mask.cpu().numpy()

    indices=adj.coalesce().indices().cpu().numpy()

    num_nodes=adj.size(0)
    row,col=indices
    maskout_self_loop=row !=col
    row=row[maskout_self_loop]
    col=col[maskout_self_loop]
    sparse_adj=sp.csr_matrix((np.ones(len(row)),(row,col)),shape=(num_nodes,num_nodes),dtype=np.float32)
    filter_adj=sparse_adj[mask,:][:,mask]
    return normalization(filter_adj).to(device)

'83'
class SelfAttentionPooling(nn.Module):
    def __init__(self,input_dim, keep_ration ,activation=torch.tanh):
        super(SelfAttentionPooling,self).__init__()
        self.input_dim=input_dim
        self.keep_ratio=keep_ration
        self.activation=activation
        self.attn_gcn=GraphConvolution(input_dim,1)


    def forward(self, adj,input_feature,graph_indicator):
        att_score=self.attn_gcn(adj,input_feature).squeeze()
        att_score=self.activation(att_score)

        mask=top_rank(att_score,graph_indicator,self.keep_ratio)

        hidden=input_feature[mask]*att_score[mask].view(-1,1)
        mask_graph_indicator=graph_indicator[mask]
        mask_adj=filter_adj(adj,mask)

        return hidden, mask_graph_indicator,mask_adj



'84,全局的池化'
class GraphConvolution(nn.Module):
    def __init__(self, input_dim, output_dim, use_bias=True):
        """图卷积：L*X*\theta

        Args:
        ----------
            input_dim: int
                节点输入特征的维度
            output_dim: int
                输出特征维度
            use_bias : bool, optional
                是否使用偏置
        """
        super(GraphConvolution, self).__init__()
        self.input_dim = input_dim
        self.output_dim = output_dim
        self.use_bias = use_bias
        self.weight = nn.Parameter(torch.Tensor(input_dim, output_dim))
        if self.use_bias:
            self.bias = nn.Parameter(torch.Tensor(output_dim))
        else:
            self.register_parameter('bias', None)
        self.reset_parameters()

    def reset_parameters(self):
        init.kaiming_uniform_(self.weight)
        if self.use_bias:
            init.zeros_(self.bias)

    def forward(self, adjacency, input_feature):
        """邻接矩阵是稀疏矩阵，因此在计算时使用稀疏矩阵乘法

        Args:
        -------
            adjacency: torch.sparse.FloatTensor
                邻接矩阵
            input_feature: torch.Tensor
                输入特征
        """
        support = torch.mm(input_feature, self.weight)
        output = torch.sparse.mm(adjacency, support)
        if self.use_bias:
            output += self.bias
        return output

    def __repr__(self):
        return self.__class__.__name__ + ' (' \
               + str(self.in_features) + ' -> ' \
               + str(self.out_features) + ')'

import torch_scatter
def global_mal_pool(x,graph_indicattor):
    num=graph_indicattor.max().item()+1
    return torch_scatter.scatter_max(x,graph_indicattor,dim=0, dim_size=num)[0]

def global_avg_pool(x,graph_indicattor):
    num=graph_indicattor.max().item()+1
    return torch_scatter.scatter_mean(x,graph_indicattor,dim=0, dim_size=num)


'SMALL'
class ModelA(nn.Module):
    def __init__(self,input_dim,hidden_dim,num_cls):
        super(ModelA,self).__init__()
        self.input_dim=input_dim
        self.hidden_dim=hidden_dim
        self.num_cls=num_cls

        self.gcn1=GraphConvolution(input_dim,hidden_dim)
        self.gcn2=GraphConvolution(hidden_dim,hidden_dim)
        self.gcn3=GraphConvolution(hidden_dim,hidden_dim)
        self.pool=SelfAttentionPooling(hidden_dim*3,0.5)

        self.fc1=nn.Linear(hidden_dim*3*2,hidden_dim)
        self.fc2=nn.Linear(hidden_dim,hidden_dim//2)
        self.fc3=nn.Linear(hidden_dim//2,num_cls)
    def forward(self, adj, input_feature,graph_ind) :
        gcn1=F.relu(self.gcn1(adj,input_feature))
        gcn2=F.relu(self.gcn2(adj,gcn1))
        gcn3=F.relu(self.gcn3(adj,gcn2))
        gen_feature=torch.cat((gcn1,gcn2,gcn3),dim=1)
        pool,pool_graph_ind,pool_adj=self.pool(adj,gen_feature,graph_ind)
        read_out=torch.cat((global_avg_pool(pool,pool_graph_ind)),global_mal_pool(pool,pool_graph_ind))
        fc1=F.relu(self.fc1(read_out))
        fc2=F.relu(self.fc1(fc1))
        logits=self.fc3(fc2)
        return logits


'HUGE'
class ModelB(nn.Module):
    def __init__(self,input_dim,hidden_dim,num_cls):
        super(ModelB,self).__init__()
        self.input_dim=input_dim
        self.hidden_dim=hidden_dim
        self.num_cls=num_cls

        self.gcn1=GraphConvolution(input_dim,hidden_dim)
        self.pool1=SelfAttentionPooling(hidden_dim,0.5)

        self.gcn2=GraphConvolution(hidden_dim,hidden_dim)
        self.pool2=SelfAttentionPooling(hidden_dim,0.5)

        self.gcn3=GraphConvolution(hidden_dim,hidden_dim)
        self.pool3=SelfAttentionPooling(hidden_dim,0.5)


        self.mlp=nn.Sequential(nn.Linear(hidden_dim*2,hidden_dim),nn.ReLU(),
                               nn.Linear(hidden_dim,hidden_dim//2),nn.ReLU(),
                               nn.Linear(hidden_dim,num_cls))

    def forward(self, adj, input_feature,graph_ind) :
        gcn1=F.relu(self.gcn1(adj,input_feature))
        pool1,pool1_graph_ind,pool1_adj=self.pool1(adj,gcn1,graph_ind)
        global_pool1 =torch.cat([global_avg_pool(pool1,pool1_graph_ind),global_mal_pool(pool1,pool1_graph_ind)],dim=1)

        gcn2 = F.relu(self.gcn2(pool1_adj, pool1))
        pool2, pool2_graph_ind, pool2_adj = self.pool2(pool1_adj, gcn2, pool1_graph_ind)
        global_pool2 = torch.cat([global_avg_pool(pool2, pool2_graph_ind), global_mal_pool(pool2, pool2_graph_ind)],
                                 dim=1)

        gcn3 = F.relu(self.gcn3(pool2_adj, pool2))
        pool3, pool3_graph_ind, pool3_adj = self.pool3(pool2_adj, gcn3, pool2_graph_ind)
        global_pool3 = torch.cat([global_avg_pool(pool3, pool3_graph_ind), global_mal_pool(pool3, pool3_graph_ind)],
                                 dim=1)

        read_out=global_pool1+global_pool2+global_pool3
        logits=self.mlp(read_out)

        return logits

